{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:47:06.789867Z",
     "end_time": "2023-06-13T10:47:06.818402Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:05:07.823988Z",
     "end_time": "2023-06-13T11:05:07.888995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "df = df.drop(\"Name\", axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:05:20.643132Z",
     "end_time": "2023-06-13T11:05:20.664129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def estimate_model(my_model, x_train, x_test, y_train, y_test):\n",
    "    pred = my_model.predict(x_test)\n",
    "    rmse = (np.sqrt(mean_squared_error(y_test, pred)))\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    score = my_model.score(x_test, y_test)\n",
    "    local_score = my_model.score(x_train, y_train)\n",
    "    print(\"Testing performance\")\n",
    "    print(\"RMSE: {:.2f}\".format(rmse))\n",
    "    print(\"R2: {:.2f}\".format(r2))\n",
    "    print(\"Score: {:.4f}\".format(score))\n",
    "    print(\"Local Score: {:.4f}\".format(local_score))\n",
    "\n",
    "    print(\"Best params: \", my_model.get_params())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def predict_by_catboost(column):\n",
    "    mydf = df.copy().drop(\"PassengerId\", axis=1)\n",
    "    mydf = mydf.drop(\"Transported\", axis=1)\n",
    "    data_x, data_y = mydf[mydf[column].isna() == False].drop(column, axis=1), mydf[mydf[column].isna() == False][column]\n",
    "    #print(data_x, data_y)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    data_x = vectorizer.fit_transform(data_x).toarray()\n",
    "    print(data_x)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y)\n",
    "    model = CatBoostClassifier()\n",
    "    param_grid = {\n",
    "    'iterations': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'depth': [3, 5, 7]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_result = grid_search.fit(x_train, y_train)\n",
    "    estimate_model(grid_result)\n",
    "    need_predict = mydf[mydf[column].isna()].drop(column, axis=1)\n",
    "    pred = grid_search.best_estimator_.predict(need_predict)\n",
    "    pred = vectorizer.fit_transform(pred).toarray()\n",
    "    print(pred)\n",
    "    mydf[mydf[column].isna() == False][column] = pred\n",
    "    return mydf\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:44:58.409712Z",
     "end_time": "2023-06-13T10:44:58.897331Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Predict Crysleep**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:19:49.013702Z",
     "end_time": "2023-06-13T10:19:49.045484Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11, 8476]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m newdf \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_by_catboost\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCryoSleep\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[51], line 9\u001B[0m, in \u001B[0;36mpredict_by_catboost\u001B[1;34m(column)\u001B[0m\n\u001B[0;32m      7\u001B[0m data_x \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(data_x)\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(data_x)\n\u001B[1;32m----> 9\u001B[0m x_train, x_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_y\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m CatBoostClassifier()\n\u001B[0;32m     11\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124miterations\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m300\u001B[39m],\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdepth\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m7\u001B[39m]\n\u001B[0;32m     15\u001B[0m }\n",
      "File \u001B[1;32mE:\\Code works\\Data Science\\Competitions_Vladdoos\\ML_Competitions\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2557\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2559\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2561\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2562\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[0;32m   2563\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[0;32m   2564\u001B[0m )\n",
      "File \u001B[1;32mE:\\Code works\\Data Science\\Competitions_Vladdoos\\ML_Competitions\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:443\u001B[0m, in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[0;32m    425\u001B[0m \n\u001B[0;32m    426\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    442\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[1;32m--> 443\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mE:\\Code works\\Data Science\\Competitions_Vladdoos\\ML_Competitions\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [11, 8476]"
     ]
    }
   ],
   "source": [
    "newdf = predict_by_catboost(\"CryoSleep\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:18:43.833474Z",
     "end_time": "2023-06-13T10:18:43.865607Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prepare data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor, StackingClassifier, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def predict_nulls(df):\n",
    "    # Create new columns based on Cabin\n",
    "    df['Deck'], df['Num'], df['Side'] = df['Cabin'].str.split('/', 2).str\n",
    "    df = df.drop(['Cabin'], axis=1)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['HomePlanet'] = df['HomePlanet'].astype(str)\n",
    "    df['HomePlanet'] = le.fit_transform(df['HomePlanet'])\n",
    "\n",
    "    df['Destination'] = df['Destination'].astype(str)\n",
    "    df['Destination'] = le.fit_transform(df['Destination'])\n",
    "\n",
    "    df['CryoSleep'] = df['CryoSleep'].astype(str)\n",
    "    df['CryoSleep'] = le.fit_transform(df['CryoSleep'])\n",
    "\n",
    "    df['VIP'] = df['VIP'].astype(str)\n",
    "    df['VIP'] = le.fit_transform(df['VIP'])\n",
    "\n",
    "    df['Deck'] = df['Deck'].astype(str)\n",
    "    df['Deck'] = le.fit_transform(df['Deck'])\n",
    "\n",
    "    df['Side'] = df['Side'].astype(str)\n",
    "    df['Side'] = le.fit_transform(df['Side'])\n",
    "\n",
    "    # Define the base models and the meta learner for the stacking classifiers/regressors\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('hgbc', HistGradientBoostingClassifier())\n",
    "    ]\n",
    "    meta_learner = LogisticRegression()\n",
    "\n",
    "    base_models_reg = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
    "        ('knn', KNeighborsRegressor()),\n",
    "        ('hgbr', HistGradientBoostingRegressor())\n",
    "    ]\n",
    "    meta_learner_reg = LinearRegression()\n",
    "\n",
    "    # Loop over columns to fill null values\n",
    "    for column in df.columns:\n",
    "        print(col)\n",
    "        # Do not try to predict nulls for 'Transported' or 'PassengerId'\n",
    "        if column not in ['Transported', 'PassengerId']:\n",
    "            # Check if column has null values\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                # Split data into sets with known and unknown column values\n",
    "                known = df[df[column].notnull()]\n",
    "                unknown = df[df[column].isnull()]\n",
    "\n",
    "                # Split features and labels\n",
    "                y_known = known[column]\n",
    "                X_known = known.drop([column], axis=1)\n",
    "\n",
    "                # Check if the column is a float (and therefore a regression problem)\n",
    "                if df[column].dtype == 'float64':\n",
    "                    # Initialize and fit a StackingRegressor\n",
    "                    stacking_reg = StackingRegressor(estimators=base_models_reg, final_estimator=meta_learner_reg)\n",
    "                    stacking_reg.fit(X_known, y_known)\n",
    "\n",
    "                    # Predict missing values and fill in the original dataframe\n",
    "                    df.loc[(df[column].isnull()), column] = stacking_reg.predict(unknown.drop([column], axis=1))\n",
    "\n",
    "                else:  # The problem is a classification one\n",
    "                    stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_learner)\n",
    "                    stacking_clf.fit(X_known, y_known)\n",
    "\n",
    "                    # Predict missing values and fill in the original dataframe\n",
    "                    df.loc[(df[column].isnull()), column] = stacking_clf.predict((unknown.drop([column], axis=1))\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:03:53.798504Z",
     "end_time": "2023-06-13T11:03:53.853505Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "predicted_df = predict_nulls(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:05:45.753649Z",
     "end_time": "2023-06-13T11:06:03.733321Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "     PassengerId  HomePlanet  CryoSleep  Destination        Age  VIP  \\\n0        0013_01           0          1            2  27.000000    0   \n1        0018_01           0          0            2  19.000000    0   \n2        0019_01           1          1            0  31.000000    0   \n3        0021_01           1          0            2  38.000000    0   \n4        0023_01           0          0            2  20.000000    0   \n...          ...         ...        ...          ...        ...  ...   \n4272     9266_02           0          1            2  34.000000    0   \n4273     9269_01           0          0            2  42.000000    0   \n4274     9271_01           2          1            0  25.030415    0   \n4275     9273_01           1          0            3  41.120857    0   \n4276     9277_01           0          1            1  43.000000    0   \n\n      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  Deck  Num  Side  \n0             0.0        0.0           0.0     0.0     0.0     0    1     0  \n1             0.0        9.0           0.0  2823.0     0.0     0    1     0  \n2             0.0        0.0           0.0     0.0     0.0     0    1     0  \n3             0.0     6652.0           0.0   181.0   585.0     0    1     0  \n4            10.0        0.0         635.0     0.0     0.0     0    1     0  \n...           ...        ...           ...     ...     ...   ...  ...   ...  \n4272          0.0        0.0           0.0     0.0     0.0     0    1     0  \n4273          0.0      847.0          17.0    10.0   144.0     0    1     0  \n4274          0.0        0.0           0.0     0.0     0.0     0    1     0  \n4275          0.0     2680.0           0.0     0.0   523.0     0    1     0  \n4276          0.0        0.0           0.0     0.0     0.0     0    1     0  \n\n[4277 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>HomePlanet</th>\n      <th>CryoSleep</th>\n      <th>Destination</th>\n      <th>Age</th>\n      <th>VIP</th>\n      <th>RoomService</th>\n      <th>FoodCourt</th>\n      <th>ShoppingMall</th>\n      <th>Spa</th>\n      <th>VRDeck</th>\n      <th>Deck</th>\n      <th>Num</th>\n      <th>Side</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0013_01</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>27.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0018_01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>19.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2823.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0019_01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>31.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0021_01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>38.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>6652.0</td>\n      <td>0.0</td>\n      <td>181.0</td>\n      <td>585.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0023_01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>20.000000</td>\n      <td>0</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>635.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4272</th>\n      <td>9266_02</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>34.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4273</th>\n      <td>9269_01</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>42.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>847.0</td>\n      <td>17.0</td>\n      <td>10.0</td>\n      <td>144.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4274</th>\n      <td>9271_01</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>25.030415</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4275</th>\n      <td>9273_01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>41.120857</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2680.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>523.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4276</th>\n      <td>9277_01</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>43.000000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4277 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:06:06.537324Z",
     "end_time": "2023-06-13T11:06:06.575325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "predicted_df.to_csv(\"features/Test_data_gpt3predcited.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:06:12.188670Z",
     "end_time": "2023-06-13T11:06:12.231202Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
